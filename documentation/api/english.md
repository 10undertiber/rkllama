# RK-LLama REST API Documentation

## Base URL

```
http://localhost:8080/
```

## API Endpoints

### 1. **GET /models**

#### Description
Returns a list of available models in the `~/RKLLAMA/models` directory.

#### Request

```http
GET /models
```

#### Response
- **Code 200**: List of available models.
  ```json
  {
    "models": [
      "model1.rkllm",
      "model2.rkllm",
      "model3.rkllm"
    ]
  }
  ```

- **Code 500**: If the `~/RKLLAMA/models` directory is not found.
  ```json
  {
    "error": "The ~/RKLLAMA/models directory is not found."
  }
  ```

---

### 2. **POST /load_model**

#### Description
Loads a specific model into memory. The model name is provided in the request body.

#### Request

```http
POST /load_model
Content-Type: application/json
```

#### Request Parameters
```json
{
  "model_name": "model_name.rkllm"
}
```

#### Response
- **Code 200**: Model successfully loaded.
  ```json
  {
    "message": "Model <model_name> loaded successfully."
  }
  ```

- **Code 400**: If a model is already loaded or if a parameter is missing.
  ```json
  {
    "error": "A model is already loaded. Please unload it first."
  }
  ```

- **Code 400**: If the specified model is not found.
  ```json
  {
    "error": "Model <model_name> not found in the /models directory."
  }
  ```

---

### 3. **POST /unload_model**

#### Description
Unloads the currently loaded model.

#### Request

```http
POST /unload_model
```

#### Response
- **Code 200**: Model successfully unloaded.
  ```json
  {
    "message": "Model unloaded successfully."
  }
  ```

- **Code 400**: If no model is currently loaded.
  ```json
  {
    "error": "No model is currently loaded."
  }
  ```

---

### 4. **GET /current_model**

#### Description
Returns the name of the currently loaded model.

#### Request

```http
GET /current_model
```

#### Response
- **Code 200**: Currently loaded model.
  ```json
  {
    "model_name": "model_name"
  }
  ```

- **Code 404**: If no model is currently loaded.
  ```json
  {
    "error": "No model is currently loaded."
  }
  ```

---

### 5. **POST /generate**

#### Description
Sends a request to generate output using the currently loaded model. The request body may include data required for processing by the model.

#### Request

```http
POST /generate
Content-Type: application/json
```

#### Request Parameters
```js
{
  "messages": "prompt or chat_template",
  "stream"  : Boolean(true || false)
}
```

#### Response
- **Code 200**: Output generated by the model.
  ```json
  {
        "id": "rkllm_chat",
        "object": "rkllm_chat",
        "created": null,
        "choices": [{
            "role": "assistant",
            "content": "output_rkllama",
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": null,
            "completion_tokens": null,
            "total_tokens": null
        }
    }
  ```

- **Code 400**: If no model is currently loaded.
  ```json
  {
    "error": "No model is currently loaded."
  }
  ```

---

### 6. **GET /**

#### Description
Default route that displays a welcome message and a link to the GitHub project.

#### Request

```http
GET /
```

#### Response
- **Code 200**: Welcome message and GitHub link.
  ```json
  {
    "message": "Welcome to RK-LLama!",
    "github": "https://github.com/notpunhnox/rk-llama"
  }
  ```

---

## Error Handling

API errors follow standard HTTP codes with detailed messages in the response body:

- **400 (Bad Request)**: Error due to a bad request (missing parameters, model already loaded, etc.).
- **404 (Not Found)**: The requested model does not exist, or no model is loaded.
- **500 (Internal Server Error)**: Server error (directory not found, internal issue).

Possible error examples:

- **400**: 
  ```json
  {
    "error": "Please provide the model name to load."
  }
  ```

- **404**: 
  ```json
  {
    "error": "No model is currently loaded."
  }
  ```

- **500**: 
  ```json
  {
    "error": "The ~/RKLLAMA/models directory is not found."
  }
  ```